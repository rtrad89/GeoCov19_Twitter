---
title: "5G Analysis of COVID-19 Tweets"
author: "RTRAD"
date: "10/11/2020"
output: 
  html_document: 
    fig_width: 10
    fig_height: 10
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      dev = "svg")

# Libraries
library(readr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggeasy)
library(tm)
library(wordcloud)
library(textclean)
library(cowplot)

theme_set(theme_linedraw())
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))
          
# Control variable
RECOMBINE = FALSE
DIR_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "annotated")
FNAME = "annotated_tweets.csv"
FPATH = file.path(DIR_PATH, FNAME)

RESAVE_TDMs = TRUE  # TRUE takes a LOT of time!
DIR_TDMs_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "preprocessed_tdms")
```

Loading the data:

```{r, make and load the data after skipping NA lables}
if (RECOMBINE)
{
  FNAME.Feb01 = "annotated_original_200201.csv"
  FPATH.Feb01 = file.path(DIR_PATH, FNAME.Feb01)
  
  FNAME.Feb15 = "annotated_original_200215.csv"
  FPATH.Feb15 = file.path(DIR_PATH, FNAME.Feb15)
  
  FNAME.Mar01 = "annotated_original_200301.csv"
  FPATH.Mar01 = file.path(DIR_PATH, FNAME.Mar01)
  
  FNAME.Apr01 = "annotated_original_200401.csv"
  FPATH.Apr01 = file.path(DIR_PATH, FNAME.Apr01)
  
  FNAME.May01 = "annotated_original_200501.csv"
  FPATH.May01 = file.path(DIR_PATH, FNAME.May01)

  FNAME.Mar15 = "annotated_original_200315.csv"
  FPATH.Mar15 = file.path(DIR_PATH, FNAME.Mar15)
  
  FNAME.Apr15 = "annotated_original_200415.csv"
  FPATH.Apr15 = file.path(DIR_PATH, FNAME.Apr15)
  
  fpaths = c(FPATH.Feb01, FPATH.Feb15, FPATH.Mar01, FPATH.Mar15,
             FPATH.Apr01, FPATH.Apr15, FPATH.May01)
  
  tweets = tibble::tibble()
  for (fp in fpaths)
  {
    temp = read_csv(fp,
                    col_types = cols(id = col_character()))
    temp$src = paste0(
      month.abb[as.numeric(
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 3, 4))
      ],
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 5, 6)
    )
  
  if(nrow(tweets) == 0)
    tweets = temp
  else
    tweets = rbind(tweets, temp)
  
  rm(temp)
  }
  
  # Drop tweets where label is NA
  tweets = tweets[!is.na(tweets$five_g),]
  

  tweets$src = factor(x = tweets$src,
                      ordered = TRUE,
                      levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01"))
  
  # Save tweets to disk
  write_csv(tweets, file = file.path(DIR_PATH, FNAME))
} else {
  tweets = read_csv(FPATH, col_types = cols(
    id = col_character(),
    src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
}
```

# General Analysis

As we found that English tweets pervade the datasets, we sufficed with them. Including other languages didn't drastically change the outputs due to that.

Previewing the data:

```{r}
# TODO change the scale itself rather than rounding
# cf. https://stackoverflow.com/questions/52602503/display-an-axis-value-in-millions-in-ggplot
plot_tweets = 
  tweets %>%
  group_by(src) %>%
  count(src) %>%
  ggplot(., aes(y = n, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(round(n/1000, 1), big.mark = ","))) +
  ylim(0, 2100000) +
  ggtitle("Number of original Eng tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; N = ",
                           format(nrow(tweets),big.mark = ",")))

ggsave(filename = paste0("general_analysis_all_tweets.pdf"),
       path = "./figures/")
print(plot_tweets)
  

tweets %>%
  group_by(src) %>%
  summarise(n_users = n_distinct(user_screen_name)) %>%
  ggplot(., aes(y = n_users, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(n_users, big.mark = ","))) +
  ggtitle("Number of users of original Eng tweets",
          subtitle = paste("Re-tweets were excluded; N = ",
                           format(n_distinct(tweets$user_screen_name),
                                  big.mark = ",")))
ggsave(filename = paste0("general_analysis_all_tweetusers.pdf"),
         path = "./figures/")

```


```{r generating users statistics}
if (file.exists(file.path(".", "usn_statistics.csv")))
{
  # Load the users statistics to save time
  usn = read_csv(file.path(".", "usn_statistics.csv"),
                 col_types = cols(src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
  
} else {
  # Make the user statistics and save it to disk
  usn = tweets %>%
    group_by(src, user_screen_name) %>%
    count(user_screen_name) %>%
    arrange(desc(n)) %>%
    ungroup(user_screen_name) %>%
    slice_max(n, n = 25) %>%
    ungroup
  
  write.csv(usn, file = file.path(".", "usn_statistics.csv"),
            row.names = FALSE)
}

# Visualise
# TODO: combine these plots into smaller grid ones
for (val in unique(usn$src))
{
  data = usn %>% filter(src == val)
  
  data %>%
    select(user_screen_name, n)
  plt =
    ggplot(data, aes(x = reorder(user_screen_name, -n), y=n)) +
    geom_bar(stat = "identity") +
    ggtitle(paste("Top 25 tweeting frequencies by users in", val)) +
    xlab("users") +
    ylim(0, 7500) +
    easy_remove_x_axis(what = "text")
    
  ggsave(filename = paste0("general_analysistop_users_",val,".pdf"),
         path = "./figures/")

  print(plt)
  rm(plt)
}

```

# 5G analysis

```{r functions definition}
# Unify 5g synonyms
five5_synonyms = c("5g,","5g.","5g?")
unify_5g = function(x)
{
  stringi::stri_replace_all_fixed(str = x,
                                    five5_synonyms,
                                    "5g",
                                    vectorise_all = F)
}

unify_5g2 = function(x) gsub("\\b5g\\b", " 5g ", x)


# Unify coronavirus synonyms
# Pay attention that the order is critical
coronavirus_synonyms = c("corona virus","covid 19", "covid19", "coronavirus19",
                         "covid", "coronavirusoutbreak", "corona ")
unify_coronavirus = function(x)
{
  stringi::stri_replace_all_fixed(str = x,
                                  coronavirus_synonyms,
                                  "coronavirus",
                                  vectorise_all = F)
}

replacePunctuation = content_transformer(function(x) {
  return (gsub("[[:punct:]]"," ", x))
  })

set.seed(13712)
  
# Run the analysis for the combination of datasets
preprocess_texts = function(texts, marker)
{
  # Communicate what basic preprocessing did:
  print(paste(
    "URLs, smilies, emojies and mentions had been removed from dataset:",
    marker))
  
  # Some cleaning:
  
  # Lower-casing
  texts  = tm_map(texts, content_transformer(tolower))
  print(paste("Lowercasing done for dataset:", marker))
  
  # # Remove hash-tags
  # texts = tm_map(texts, content_transformer(replace_hash))
  # print(paste("Hashtags removed from dataset:", marker))
  
  # Replace punctuation with spaces
  texts = tm_map(texts, replacePunctuation)
  print(paste("Punctuation replaced with spaces in dataset:", marker))
  
  # Unify 5G variants
  # texts = tm_map(texts, content_transformer(unify_5g))
  texts = tm_map(texts, content_transformer(unify_5g2))
  print(paste("5g variants (5g! and the likes) were standardised in", marker))
  
  # Remove stand-alone numbers to get rid of 19
  # # function source: https://stackoverflow.com/a/23866586/3429115
  remove_alone_nbr = function (x)
    gsub('\\s*(?<!\\B|-)\\d+(?!\\B|-)\\s*', " ", x, perl=TRUE)
  texts = tm_map(texts, content_transformer(remove_alone_nbr))
  print(paste("Standalone numbers removed from dataset:", marker))
  
  # Unify Coronavirus synonyms
  texts = tm_map(texts, content_transformer(unify_coronavirus))
  print(paste("Coronavirus synonyms were unified in",
              marker))
  print(coronavirus_synonyms)
  
  # Removing English stop-words
  texts = tm_map(texts, removeWords, stopwords("english"))
  print(paste("English stopwords removed from dataset:", marker))
  
  # Removing excessive white spaces
  texts = tm_map(texts, stripWhitespace)
  print(paste("White spaces normalised for dataset:", marker))
  
  # Stemming the terms
  texts = tm_map(texts, stemDocument, language = "english")
  print(paste("Snowball Stemming done for dataset:", marker))
  
  return(texts)
}

compute_salience = function(tdm.all, tdm.5g, verbose = F)
{
  if(verbose)
  {
    print(tdm.all)
    print(tdm.5g)
  }
  
  t.and.fiveG.DocCounts = (rowSums(as.matrix(tdm.5g) > 0))
  
  #print("Sufficing with terms whose sparsity doesn't exceed .993% in all")
  #tdm.all = removeSparseTerms(x = tdm.all, sparse = .99)
  
  t.counts = (rowSums(as.matrix(
    tdm.all[tdm.all$dimnames$Terms %in% names(t.and.fiveG.DocCounts),]) > 0)
  )
    
  # Supply deleted term counts due to sparsity processing with NA
  t.counts = t.counts[names(t.and.fiveG.DocCounts)]
  
  fiveG.counts = as.double(length(tdm.5g$dimnames$Docs))
  tweets.counts = as.double(length(tdm.all$dimnames$Docs))
  
  # Calculate the conditional probabilities
  Pr.t.given.5g = t.and.fiveG.DocCounts / fiveG.counts
  Pr.t = t.counts / tweets.counts
  Pr.boost = Pr.t.given.5g - Pr.t
  
  return(cbind(Pr.t, Pr.t.given.5g, Pr.boost))
}
```

```{r}
smry = summary(tweets$five_g)
```

There are `r round(100 * as.numeric(smry["TRUE"]) / nrow(tweets), 2)`% 5G tweets in the hydrated datasets.

```{r, fig.height=12}
plt1 =
  tweets %>%
  filter(!five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/1000, 1),
                                            big.mark = ",")),
             size = 3, vjust = 0.0) +
  ggtitle("Number of non-5G tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; N =",
                           format(nrow(subset(tweets, !five_g)), big.mark=","))) +
  ylim(0, 2100000) +
  easy_remove_y_axis() +
  easy_text_size(which = "plot.title", size = 12)


plt2 = 
  tweets %>%
  filter(five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/1000, 1),
                                            big.mark = ",")),
             size = 3, vjust = 0.0) +
  ggtitle("Number of 5G tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; N =",
                           format(nrow(subset(tweets, five_g)),big.mark=","))) +
  ylim(0, 2100000) +
  easy_remove_y_axis() +
  easy_text_size(which = "plot.title", size = 12)

mddl_row = plot_grid(plt1, plt2)

plt3 =
  tweets %>%
  group_by(src) %>%
  summarise(fraction = sum(five_g)/n()) %>%
  ggplot(., aes(x = src, y = fraction)) +
  geom_bar(stat = "identity") +
  geom_text(stat="identity", aes(label=paste0(round(100*fraction, 2), "%")),
            size = 5, hjust = 0.5, vjust = 1,
            position = position_dodge2(width = 0), colour = "white") +
  ggtitle("Percentage of 5G tweets hydrated",
          subtitle = paste("Re-tweets were excluded; N =",
                           format(nrow(tweets), big.mark = ",")))+
  easy_remove_y_axis()



plot_grid(plot_tweets + easy_remove_y_axis(),
          mddl_row,
          plt3,
          ncol = 1)

ggsave(filename = paste0("general_analysis_hydrations.pdf"),
         path = "./figures/")

rm(plot_tweets, plt1, plt2, plt3, mddl_row)
```

Concentrating on 5g Tweets after preprocessing:

```{r combined dataset analysis}
# Separate the two categories to control for anoy confounding effects

five_g_tweets = tweets %>%
  filter(five_g)

non_5g_tweets = tweets %>%
  filter(!five_g)

#rm(tweets)

set.seed(13712)


val="all_combined"
if(RESAVE_TDMs)
{
  texts = VCorpus(VectorSource(five_g_tweets$text))
  texts = preprocess_texts(texts, val)
  # Building the term-doc matrix
  dtm_tweets = TermDocumentMatrix(texts,
                                  control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_tweets, file = file.path(DIR_TDMs_PATH,
                                                paste0(val,"_5G", ".rds")))
  
  n5g_texts = VCorpus(VectorSource(non_5g_tweets$text))
  n5g_texts = preprocess_texts(n5g_texts, val)
  # Building the term-doc matrix
  dtm_n5g_tweets = TermDocumentMatrix(n5g_texts,
                                      control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_n5g_tweets, file = file.path(DIR_TDMs_PATH,
                                                paste0(val,"_n5G", ".rds")))
  
  # MEMORY INSUFFICIENT!
  all_texts = VCorpus(VectorSource(tweets$text))
  all_texts = preprocess_texts(all_texts, val)
  # Building the term-doc matrix
  dtm_all_tweets = TermDocumentMatrix(all_texts,
                                      control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_all_tweets, file = file.path(DIR_TDMs_PATH,
                                                    paste0(val, ".rds")))
  
} else {
  
  # Load the TDM
  dtm_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                        paste0(val,"_5G", ".rds")))
  # dtm_n5g_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
  #                                       paste0(val,"_n5G", ".rds")))
  dtm_all_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                           paste0(val, ".rds")))
}
```

After we remove any terms which are sparse more than 98% of the times, we separate the two datasets to control for confounding effects.

```{r unpacking combined data}
# Removing sparsity!
val = "non_5G"
print("Due to technical problems, we are removing terms whose sparsity exceed 99% but in 5G datasets")
dtm_all_tweets = removeSparseTerms(x = dtm_all_tweets, sparse = 0.99)
dtm_n5g_tweets = removeSparseTerms(x = dtm_n5g_tweets, sparse = 0.99)

# Make the general wordcloud for non 5g terms
m <- as.matrix(dtm_n5g_tweets)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=unname(v))

vDoc = sort(rowSums(m > 0),decreasing=TRUE)
dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))

p = ggplot(d[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("occurrences") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in Non-5G Tweets (Excluding Hashtags)")) +
coord_flip()
print(p)

ggsave(filename = paste0("general_analysis_CorpusOcurrences",val,".pdf"),
         path = "./figures/")

p = ggplot(dDoc[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("tweets") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in in Non-5G Tweets (Excluding Hashtags)")) +
coord_flip()

ggsave(filename = paste0("general_analysis_TweetOcurrences",val,".pdf"),
         path = "./figures/")

print(p)
rm(p)

print("removing coronavirus term: done")
d = d[!d$word == "coronavirus", ]
wordcloud(words = d$word, freq = d$freq,
          min.freq = 3, random.order=FALSE,
          max.words=200, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
rm(m, v, d, dDoc, vDoc)

# Make the wordclouds of 5G only

val = "5G"
m <- as.matrix(dtm_tweets)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=unname(v))

vDoc = sort(rowSums(m > 0),decreasing=TRUE)
dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))

p = ggplot(d[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("occurrences") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in", val, " Tweets (Excluding Hashtags)")) +
coord_flip()
print(p)

ggsave(filename = paste0("general_analysis_CorpusOcurrences",val,".pdf"),
         path = "./figures/")

p = ggplot(dDoc[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("tweets") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in", val, "(Excluding Hashtags)")) +
coord_flip()

ggsave(filename = paste0("general_analysis_TweetOcurrences",val,".pdf"),
         path = "./figures/")

print(p)
rm(p)

print("Pearson Correlation among 5g and other terms (threshold = 0.1):")
print(findAssocs(dtm_tweets, terms = c("5g"), corlimit = .1))

set.seed(13712)

print("Salience and conditional probabilities: on the lab report due to RAM")
# salience = compute_salience(tdm.all = dtm_all_tweets,
                            # tdm.5g = removeSparseTerms(dtm_tweets, sparse = 0.99))
# Exclude "5g" if there
# if("5g" %in% names(salience))
#   salience["5g",] = NA
# ggplot(data = data.frame(salience),
#        aes(x = "Pr.boost",y = rownames(salience))) +
#   geom_bar(stat = "identity")
  
# barplot(sort(salience[, "Pr.boost"], decreasing = T)[0:30],
#         las = 2, cex.names = 0.8,
#         main ="Top 30 terms whose conditional prob. to occur\nin a tweet given '5g' exceed their marginal prob.")

# 
# layout(matrix(c(1, 2), nrow=2), heights=c(10, 90))
# par(mar=rep(0, 4))
# plot.new()
# text(x=0.5, y=0.5, val, font = 2, cex = 1)
print("removing 5G and coronavirus terms done")
d = d[!d$word %in% c("5g", "coronavirus"), ]
wordcloud(words = d$word, freq = d$freq,
          min.freq = 3, random.order=FALSE,
          max.words=200, rot.per=0.35, colors=brewer.pal(8, "Dark2"))

rm(dtm_tweets, dtm_all_tweets)
```

```{r, iterating over datasets}
# Iterate to produce the word-clouds all of the datasets one by one
# SAve the 50 most frequent terms in each dataset
freq.terms = data.frame()
for (val in unique(five_g_tweets$src))
{
  if(RESAVE_TDMs)
  {
    texts = VCorpus(VectorSource(subset(five_g_tweets, src == val)$text))
    texts = preprocess_texts(texts, val)
    # Building the term-doc matrix
    dtm_tweets = TermDocumentMatrix(texts,
                                    control = list(wordLengths=c(2, Inf)))
    saveRDS(object = dtm_tweets, file = file.path(DIR_TDMs_PATH,
                                                  paste0(val,"_5G", ".rds")))

    # All tweets now
    # We don't need to compute salience now
    
    # texts = VCorpus(VectorSource(subset(tweets, src == val)$text))
    # texts = preprocess_texts(texts, val)
    # # Building the term-doc matrix
    # dtm_all_tweets = TermDocumentMatrix(texts,
    #                                     control = list(wordLengths=c(2, Inf)))
    # saveRDS(object = dtm_all_tweets, file = file.path(DIR_TDMs_PATH,
    #                                                   paste0(val, ".rds")))
  } else {
    # Loading the term-doc matrices
    dtm_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                          paste0(val,"_5G", ".rds")))
    # We don't need to compute salience now
    # dtm_all_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
    #                                       paste0(val, ".rds")))
  }
  
  m = as.matrix(dtm_tweets)
  
  # Corpus counts
  v = sort(rowSums(m),decreasing=TRUE)
  d = data.frame(word = names(v),freq=unname(v))
  
  # Tweets counts
  vDoc = sort(rowSums(m > 0),decreasing=TRUE)
  dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))
  dDoc[, "src"] = val
  # Append the frequent terms to the repository
  if(nrow(freq.terms) == 0)
    freq.terms = dDoc
  else
    freq.terms = rbind(freq.terms, dDoc)

  p = ggplot(d[1:50, ],
       aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity") +
  xlab("frequent words") +
  ylab("occurrences") +
  #easy_rotate_x_labels(angle = 90) +
  ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in", val, "(Excluding Hashtags)")) +
  coord_flip()
  
  ggsave(filename = paste0("general_analysis_CorpusOcurrences",val,".pdf"),
         path = "./figures/")
  
  print(p)

  p = ggplot(dDoc[1:50, ],
       aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity") +
  xlab("frequent words") +
  ylab("tweets") +
  #easy_rotate_x_labels(angle = 90) +
  ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in ", val, "(Excluding Hashtags)")) +
  coord_flip()
  print(p)
  rm(p)

  ggsave(filename = paste0("general_analysis_TweetOcurrences",val,".pdf"),
         path = "./figures/")
  
  print("Pearson Correlation scores among 5g term and other terms, with a strength of 0.2 and above:")
  print(findAssocs(dtm_tweets, terms = c("5g"), corlimit = .2))
  
  print("Salience and conditional probabilities:")
  # salience = compute_salience(tdm.all = dtm_all_tweets, tdm.5g = dtm_tweets)
  # Exclude "5g" if there
  # if("5g" %in% names(salience))
  #   salience["5g",] = NA
  # ggplot(data = data.frame(salience),
  #        aes(x = "Pr.boost",y = rownames(salience))) +
  #   geom_bar(stat = "identity")
  
  # barplot(sort(salience[, "Pr.boost"], decreasing = T)[0:30],
  #         las = 2, cex.names = 0.7, main ="Top 30 terms whose conditional prob. to occur\nin a tweet given '5g' exceed their marginal prob.")
  # title(main=ttl, cex.main=0.6)
  
  # layout(matrix(c(1, 2), nrow=2), heights=c(10, 90))
  # par(mar=rep(0, 4))
  # plot.new()
  # text(x=0.5, y=0.5, val, font = 2, cex = 1)
  print("Removing 5G and Coronavirus terms")
  d = d[!d$word %in% c("5g", "coronavirus"), ]
  wordcloud(words = d$word, freq = d$freq, min.freq = 3,
            max.words=200, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  
  # NOTE: just for testing purposes while we can run it on the server
  # if(val == "Feb15")
  #   break
}

# Save the frequencies to disk
DIR_FREQ_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "freqterms")
# saveRDS(object = freq.terms, file = file.path(DIR_FREQ_PATH, "freq_terms.rds"))

```

We will calculate the probabilities of a term \(w\) given the term \(5g\):

\[P(w | 5g) = \frac{P(w \cap 5g)}{P(5g)} = \frac{C(tweets_{w\wedge5g})}{C(tweets_{5g})}\]

Where:

\[P(w \cap 5g) = \frac{C(tweets_{w\wedge5g})}{C(tweets_{all})}\]
\[P(5g) = \frac{C(tweets_{5g})}{C(tweets_{all})}\]

---

