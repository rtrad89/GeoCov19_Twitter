---
title: "5G Analysis of COVID-19 Tweets"
author: "RTRAD"
date: "16/10/2020"
output: 
  html_document: 
    fig_width: 8
    fig_height: 7
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      dev = "svg")

# Libraries
library(readr)
library(dplyr)
library(ggplot2)
library(ggeasy)
library(tm)
library(wordcloud)
library(textclean)
library(cowplot)

theme_set(theme_linedraw())
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))
          
# Control variable
RECOMBINE = FALSE
DIR_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "annotated")
FNAME = "annotated_tweets.csv"
FPATH = file.path(DIR_PATH, FNAME)
```

Loading the data:

```{r, make and load the data after skipping NA lables}
if (RECOMBINE)
{
  FNAME.Feb01 = "annotated_original_200201.csv"
  FPATH.Feb01 = file.path(DIR_PATH, FNAME.Feb01)
  
  FNAME.Feb15 = "annotated_original_200215.csv"
  FPATH.Feb15 = file.path(DIR_PATH, FNAME.Feb15)
  
  FNAME.Mar01 = "annotated_original_200301.csv"
  FPATH.Mar01 = file.path(DIR_PATH, FNAME.Mar01)
  
  FNAME.Apr01 = "annotated_original_200401.csv"
  FPATH.Apr01 = file.path(DIR_PATH, FNAME.Apr01)
  
  FNAME.May01 = "annotated_original_200501.csv"
  FPATH.May01 = file.path(DIR_PATH, FNAME.May01)

  FNAME.Mar15 = "annotated_original_200315.csv"
  FPATH.Mar15 = file.path(DIR_PATH, FNAME.Mar15)
  
  FNAME.Apr15 = "annotated_original_200415.csv"
  FPATH.Apr15 = file.path(DIR_PATH, FNAME.Apr15)
  
  fpaths = c(FPATH.Feb01, FPATH.Feb15, FPATH.Mar01, FPATH.Mar15,
             FPATH.Apr01, FPATH.Apr15, FPATH.May01)
  
  tweets = tibble::tibble()
  for (fp in fpaths)
  {
    temp = read_csv(fp,
                    col_types = cols(id = col_character()))
    temp$src = paste0(
      month.abb[as.numeric(
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 3, 4))
      ],
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 5, 6)
    )
  
  if(nrow(tweets) == 0)
    tweets = temp
  else
    tweets = rbind(tweets, temp)
  
  rm(temp)
  }
  
  # Drop tweets where label is NA
  tweets = tweets[!is.na(tweets$five_g),]
  

  tweets$src = factor(x = tweets$src,
                      ordered = TRUE,
                      levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01"))
  
  # Save tweets to disk
  write_csv(tweets, file = file.path(DIR_PATH, FNAME))
} else {
  tweets = read_csv(FPATH, col_types = cols(
    id = col_character(),
    src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
}
```

# General Analysis

Previewing the data:

```{r}
plot_tweets = 
  tweets %>%
  group_by(src) %>%
  count(src) %>%
  ggplot(., aes(y = n, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(round(n/1000, 1), big.mark = ","))) +
  ggtitle("Number of original tweets hydrated (in Thousands)",
          subtitle = "Re-tweets were excluded.")

print(plot_tweets)
  

tweets %>%
  group_by(src) %>%
  summarise(n_users = n_distinct(user_screen_name)) %>%
  ggplot(., aes(y = n_users, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(n_users, big.mark = ","))) +
  ggtitle("Number of users of original tweets",
          subtitle = "Re-tweets were excluded.")
```


```{r generating users statistics}
if (file.exists(file.path(".", "usn_statistics.csv")))
{
  # Load the users statistics to save time
  usn = read_csv(file.path(".", "usn_statistics.csv"),
                 col_types = cols(src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
  
} else {
  # Make the user statistics and save it to disk
  usn = tweets %>%
    group_by(src, user_screen_name) %>%
    count(user_screen_name) %>%
    arrange(desc(n)) %>%
    ungroup(user_screen_name) %>%
    slice_max(n, n = 25) %>%
    ungroup
  
  write.csv(usn, file = file.path(".", "usn_statistics.csv"),
            row.names = FALSE)
}

# Visualise

for (val in unique(usn$src))
{
  data = usn %>% filter(src == val)
  
  data %>%
    select(user_screen_name, n)
  plt =
    ggplot(data, aes(x = reorder(user_screen_name, -n), y=n)) +
    geom_bar(stat = "identity") +
    ggtitle(paste("Top 25 tweeting frequencies by users in", val)) +
    xlab("users") +
    easy_remove_x_axis(what = "text")
    

  print(plt)
  rm(plt)
}

```

# 5G analysis


```{r}
smry = summary(tweets$five_g)
```

There are `r round(100 * as.numeric(smry["TRUE"]) / nrow(tweets), 2)`% 5G tweets in the hydrated datasets.

```{r, fig.height=12}
plt1 =
  tweets %>%
  filter(!five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/1000, 1),
                                            big.mark = ",")),
             size = 3, vjust = 0.0) +
  ggtitle("Number of non-5G original tweets hydrated (in Thousands)",
          subtitle = "Re-tweets were excluded.") +
  easy_remove_y_axis()

plt2 = 
  tweets %>%
  filter(five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/100, 1),
                                            big.mark = ",")),
             size = 3, vjust = 0.0) +
  ggtitle("Number of 5G tweets hydrated (in Thousands)",
          subtitle = "Re-tweets were excluded.") +
  easy_remove_y_axis()

mddl_row = plot_grid(plt1, plt2)

plt3 =
  tweets %>%
  group_by(src) %>%
  summarise(fraction = sum(five_g)/n()) %>%
  ggplot(., aes(x = src, y = fraction)) +
  geom_bar(stat = "identity") +
  geom_text(stat="identity", aes(label=paste0(round(100*fraction, 2), "%")),
            size = 5, hjust = 0.5, vjust = 1,
            position = position_dodge2(width = 0), colour = "white") +
  ggtitle("Percentage of 5G tweets hydrated",
          subtitle = "Re-tweets were excluded.") +
  easy_remove_y_axis()

plot_grid(plot_tweets + easy_remove_y_axis(),
          mddl_row,
          plt3,
          ncol = 1)

rm(plot_tweets, plt1, plt2, plt3)
```

Concentrating on 5g Tweets after preprocessing:

```{r wordclouds}
five_g_tweets = tweets %>%
  filter(five_g)

# Iterate to produce the word-clouds all of them at once
for (val in unique(five_g_tweets$src))
{
  # TODO: investigate which preprocessing step is removing 5g
  data = five_g_tweets %>% filter(src == val)
  texts = VCorpus(VectorSource(data$text))
  # Communicate what basic preprocessing did:
  print(paste("URLs, smilies, emojies and mentions had been removed for dataset:", val))
  # Some cleaning:
  
  # Lower-casing
  texts  = tm_map(texts, content_transformer(tolower))
  print(paste("Lowercasing done for dataset:", val))
  
  # Remove hash-tags
  texts = tm_map(texts, content_transformer(replace_hash))
  print(paste("Hashtags removed for dataset:", val))
  
  # Unify 5g synonyms
  five5_synonyms = c("5g,","5g.","5g?")
  unify_5g = function(x)
  {
    stringi::stri_replace_all_fixed(str = x,
                                    five5_synonyms,
                                    "5g",
                                    vectorise_all = F)
  }

  unify_5g2 = content_transformer(function(x) gsub("\\b5g\\b", " 5g ", x))

  # texts = tm_map(texts, content_transformer(unify_5g))
  texts = tm_map(texts, unify_5g2)
  print(paste("5g variants (5g! and the likes) were standardised in", val))
  
  # Unify coronavirus synonyms
  coronavirus_synonyms = c("corona virus","covid 19", "coronavirus19",
                           "covid","covid19", "coronavirusoutbreak", "corona ")
  unify_coronavirus = function(x)
  {
    stringi::stri_replace_all_fixed(str = x,
                                    coronavirus_synonyms,
                                    "coronavirus",
                                    vectorise_all = F)
  }
  texts = tm_map(texts, content_transformer(unify_coronavirus))
  print(paste("Coronavirus synonyms were unified in",
              val))
  print(coronavirus_synonyms)
  
  # Removing English stop-words
  texts = tm_map(texts, removeWords, stopwords("english"))
  print(paste("English stopwords removed for dataset:", val))
  
  # And Spanish stop-words
  texts = tm_map(texts, removeWords, stopwords("spanish"))
  print(paste("Spanish stopwords removed for dataset:", val))
  
  # Remove punctuation
  texts = tm_map(texts, removePunctuation)
  print(paste("Punctuation removed for dataset:", val))
  
  # And stand-alone numbers
  # # function source: https://stackoverflow.com/a/23866586/3429115
  remove_alone_nbr = function (x)
    gsub('\\s*(?<!\\B|-)\\d+(?!\\B|-)\\s*', " ", x, perl=TRUE)
  texts = tm_map(texts, content_transformer(remove_alone_nbr))
  print(paste("Standalone numbers removed for dataset:", val))

  
  # Removing excessive white spaces
  texts = tm_map(texts, stripWhitespace)
  print(paste("White spaces normalised for dataset:", val))
  
  # Building the term-doc matrix
  dtm_tweets = TermDocumentMatrix(texts,
                                  control = list(wordLengths=c(2, Inf)))
  
  m <- as.matrix(dtm_tweets)
  v <- sort(rowSums(m),decreasing=TRUE)
  d <- data.frame(word = names(v),freq=unname(v))
  
  plt = 
    ggplot(d[1:50, ],
         aes(x = reorder(word, freq), y = freq)) +
    geom_bar(stat = "identity") +
    xlab("frequent words") +
    #easy_rotate_x_labels(angle = 90) +
    ggtitle(paste("Top 50 Frequent Words in", val, "(Excluding Hashtags)"))
  
  print(plt + coord_flip())
  rm(plt)
  
  print(findAssocs(dtm_tweets, terms = c("5g"), corlimit = .2))
  
  set.seed(13712)
  layout(matrix(c(1, 2), nrow=2), heights=c(10, 90))
  par(mar=rep(0, 4))
  plot.new()
  text(x=0.5, y=0.5, val, font = 2, cex = 1)
  wordcloud(words = d$word, freq = d$freq, min.freq = 3,
            max.words=50, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
}
```


