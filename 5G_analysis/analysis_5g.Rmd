---
title: "5G Analysis of COVID-19 Tweets"
author: "RTRAD"
date: "16/10/2020"
output: 
  html_document: 
    fig_width: 8
    fig_height: 7
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      dev = "svg")

# Libraries
library(readr)
library(dplyr)
library(ggplot2)
library(ggeasy)
library(tm)
library(wordcloud)
library(textclean)
library(cowplot)

theme_set(theme_linedraw())
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))
          
# Control variable
RECOMBINE = FALSE
# TODO: rename the folder to the original un-copied one
DIR_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "annotated - Copy")
FNAME = "annotated_tweets.csv"
FPATH = file.path(DIR_PATH, FNAME)
```

Loading the data:

```{r, make and load the data after skipping NA lables}
if (RECOMBINE)
{
  FNAME.Feb01 = "annotated_original_200201.csv"
  FPATH.Feb01 = file.path(DIR_PATH, FNAME.Feb01)
  
  FNAME.Feb15 = "annotated_original_200215.csv"
  FPATH.Feb15 = file.path(DIR_PATH, FNAME.Feb15)
  
  FNAME.Mar01 = "annotated_original_200301.csv"
  FPATH.Mar01 = file.path(DIR_PATH, FNAME.Mar01)
  
  FNAME.Apr01 = "annotated_original_200401.csv"
  FPATH.Apr01 = file.path(DIR_PATH, FNAME.Apr01)
  
  FNAME.May01 = "annotated_original_200501.csv"
  FPATH.May01 = file.path(DIR_PATH, FNAME.May01)
  
  fpaths = c(FPATH.Feb01, FPATH.Feb15, FPATH.Mar01, FPATH.Apr01, FPATH.May01)
  
  tweets = tibble::tibble()
  for (fp in fpaths)
  {
    temp = read_csv(fp,
                    col_types = cols(id = col_character()))
    temp$src = paste0(
      month.abb[as.numeric(
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 3, 4))
      ],
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 5, 6)
    )
  
  if(nrow(tweets) == 0)
    tweets = temp
  else
    tweets = rbind(tweets, temp)
  
  rm(temp)
  }
  
  # Drop tweets where label is NA
  tweets = tweets[!is.na(tweets$five_g),]
  

  tweets$src = factor(x = tweets$src,
                      ordered = TRUE,
                      levels = c("Feb01", "Feb15", "Mar01", "Apr01", "May01"))
  
  # Save tweets to disk
  write_csv(tweets, file = file.path(DIR_PATH, FNAME))
} else {
  tweets = read_csv(FPATH, col_types = cols(
    id = col_character(),
    src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Apr01", "May01")))
    )
}
```

# General Analysis

Previewing the data:

```{r}
plot_tweets = 
  tweets %>%
  group_by(src) %>%
  count(src) %>%
  ggplot(., aes(y = n, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(n, big.mark = ","))) +
  ggtitle("Number of original tweets hydrated",
          subtitle = "Re-tweets were excluded.")

print(plot_tweets)
  

tweets %>%
  group_by(src) %>%
  summarise(n_users = n_distinct(user_screen_name)) %>%
  ggplot(., aes(y = n_users, x = src)) +
  geom_bar(stat = "identity") +
  geom_label(aes(label=format(n_users, big.mark = ","))) +
  ggtitle("Number of users of original tweets",
          subtitle = "Re-tweets were excluded.")
```


```{r generating users statistics}
if (file.exists(file.path(".", "usn_statistics.csv")))
{
  # Load the users statistics to save time
  usn = read_csv(file.path(".", "usn_statistics.csv"),
                 col_types = cols(src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Apr01", "May01")))
    )
  
  
  
} else {
  # Make the user statistics and save it to disk
  usn = tweets %>%
    group_by(src, user_screen_name) %>%
    count(user_screen_name) %>%
    arrange(desc(n)) %>%
    ungroup(user_screen_name) %>%
    slice_max(n, n = 25) %>%
    ungroup
  
  write.csv(usn, file = file.path(".", "usn_statistics.csv"),
            row.names = FALSE)
}

# Visualise

for (val in unique(usn$src))
{
  data = usn %>% filter(src == val)
  
  data %>%
    select(user_screen_name, n)
  plt =
    ggplot(data, aes(x = reorder(user_screen_name, -n), y=n)) +
    geom_bar(stat = "identity") +
    ggtitle(paste("Top 25 tweeting frequencies by users in", val)) +
    xlab("users") +
    easy_remove_x_axis(what = "text")
    

  print(plt)
  rm(plt)
}

```

# 5G analysis


```{r}
smry = summary(tweets$five_g)
```

There are `r round(100 * as.numeric(smry["TRUE"]) / nrow(tweets), 2)`% 5G tweets in the hydrated datasets.

```{r}
plt1 =
  tweets %>%
  filter(!five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(
    label=format(after_stat(count),
                 big.mark = ","))) +
  ggtitle("Number of non-5G original tweets hydrated",
          subtitle = "Re-tweets were excluded.") +
  easy_remove_y_axis()

plt2 = 
  tweets %>%
  filter(five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(after_stat(count),
                                            big.mark = ","))) +
  ggtitle("Number of 5G tweets hydrated",
          subtitle = "Re-tweets were excluded.") +
  easy_remove_y_axis()

bottom_row = plot_grid(plt1, plt2)

plot_grid(plot_tweets + easy_remove_y_axis(),
          bottom_row, ncol = 1)
rm(plt1, plt2)
```

Concentrating on 5g Tweets after preprocessing:

```{r wordclouds}
five_g_tweets = tweets %>%
  filter(five_g)

# Iterate to produce the wordclouds all of them at once
for (val in unique(five_g_tweets$src))
{
  data = five_g_tweets %>% filter(src == val)
  texts = VCorpus(VectorSource(data$text))
  # Some cleaning:
  # Lowercasing
  texts  = tm_map(texts, content_transformer(tolower))
  print(paste("Lowercasing done for dataset:", val))
  # Removing English stop-words
  texts = tm_map(texts, removeWords, stopwords("english"))
  print(paste("English stopwords removed for dataset:", val))
  # And Spanish stop-words
  texts = tm_map(texts, removeWords, stopwords("spanish"))
  print(paste("Spanish stopwords removed for dataset:", val))
  # function source: https://stackoverflow.com/a/23866586/3429115
  remove_alone_nbr = function (x) 
    gsub('\\s*(?<!\\B|-)\\d+(?!\\B|-)\\s*', " ", x, perl=TRUE)
  texts = tm_map(texts, content_transformer(remove_alone_nbr))
  print(paste("Standalone numbers removed for dataset:", val))
  # Remove hashtags
  texts = tm_map(texts, content_transformer(replace_hash))
  print(paste("Hashtags removed for dataset:", val))
  # Remove URLS?
  # Removing excessive white spaces
  texts = tm_map(texts, stripWhitespace)
  print(paste("White spaces normalised for dataset:", val))
  
  # Building the term-doc matrix
  dtm_tweets = TermDocumentMatrix(texts)
  
  m <- as.matrix(dtm_tweets)
  v <- sort(rowSums(m),decreasing=TRUE)
  d <- data.frame(word = names(v),freq=v)
  
  plt = 
    ggplot(d[1:50, ],
         aes(x = reorder(word, -freq), y = freq)) +
    geom_bar(stat = "identity") +
    xlab("frequent words") +
    easy_rotate_x_labels(angle = 90) +
    ggtitle(paste("Top 50 Frequent Words in", val))
  print(plt)
  rm(plt)
  
  # TODO: replace all 5G variations with 5g
  print(findAssocs(dtm_tweets, terms = c("#5g", "5g.", "5g,"), corlimit = .2))
  
  set.seed(13712)
  layout(matrix(c(1, 2), nrow=2), heights=c(0.2, 5))
  par(mar=rep(0, 4))
  plot.new()
  text(x=0.5, y=0.5, val, font = 2, cex = 1.2)
  wordcloud(words = d$word, freq = d$freq, min.freq = 3,
            max.words=100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"),
            main="Title")
}
```


