---
title: "5G Analysis of COVID-19 Tweets"
author: "RTRAD"
date: "`r Sys.Date()`"
output: 
  html_document: 
    fig_width: 10
    fig_height: 10
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      dev = "svg")

# Libraries
library(readr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggeasy)
library(tm)
library(wordcloud)
library(textclean)
library(cowplot)

theme_set(theme_linedraw())
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))
          
# Control variable
RECOMBINE = FALSE
DIR_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "annotated")
FNAME = "annotated_tweets.csv"
FPATH = file.path(DIR_PATH, FNAME)

RESAVE_TDMs = FALSE  # For combined datasets, TRUE takes a LOT of time!
RESAVE_INDIVIDUAL_TDMs = FALSE # For individual datasets
DIR_TDMs_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "preprocessed_tdms")

MAX_SPARSITY = 0.99
```

Loading the data:

```{r, make and load the data after skipping NA lables}
if (RECOMBINE)
{
  FNAME.Feb01 = "annotated_original_200201.csv"
  FPATH.Feb01 = file.path(DIR_PATH, FNAME.Feb01)
  
  FNAME.Feb15 = "annotated_original_200215.csv"
  FPATH.Feb15 = file.path(DIR_PATH, FNAME.Feb15)
  
  FNAME.Mar01 = "annotated_original_200301.csv"
  FPATH.Mar01 = file.path(DIR_PATH, FNAME.Mar01)
  
  FNAME.Apr01 = "annotated_original_200401.csv"
  FPATH.Apr01 = file.path(DIR_PATH, FNAME.Apr01)
  
  FNAME.May01 = "annotated_original_200501.csv"
  FPATH.May01 = file.path(DIR_PATH, FNAME.May01)

  FNAME.Mar15 = "annotated_original_200315.csv"
  FPATH.Mar15 = file.path(DIR_PATH, FNAME.Mar15)
  
  FNAME.Apr15 = "annotated_original_200415.csv"
  FPATH.Apr15 = file.path(DIR_PATH, FNAME.Apr15)
  
  fpaths = c(FPATH.Feb01, FPATH.Feb15, FPATH.Mar01, FPATH.Mar15,
             FPATH.Apr01, FPATH.Apr15, FPATH.May01)
  
  tweets = tibble::tibble()
  for (fp in fpaths)
  {
    temp = read_csv(fp,
                    col_types = cols(id = col_character()))
    temp$src = paste0(
      month.abb[as.numeric(
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 3, 4))
      ],
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 5, 6)
    )
  
  if(nrow(tweets) == 0)
    tweets = temp
  else
    tweets = rbind(tweets, temp)
  
  rm(temp)
  }
  
  # Drop tweets where label is NA
  tweets = tweets[!is.na(tweets$five_g),]
  

  tweets$src = factor(x = tweets$src,
                      ordered = TRUE,
                      levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01"))
  
  # Save tweets to disk
  write_csv(tweets, file = file.path(DIR_PATH, FNAME))
} else {
  tweets = read_csv(FPATH, col_types = cols(
    id = col_character(),
    src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
}
```

# General Analysis

As we found that English tweets pervade the datasets, we sufficed with them. Including other languages didn't drastically change the outputs due to that.

Previewing the data:

```{r general plots, fig.width=8, fig.height=5}
# TODO change the scale itself rather than rounding
# cf. https://stackoverflow.com/questions/52602503/display-an-axis-value-in-millions-in-ggplot
tots = tweets %>%
  group_by(src) %>%
  count() %>%
  rename(total = n)

plot_tweets = 
  tweets %>%
  group_by(src, tagged=!is.na(hashtags)) %>%
  count() %>%
  ggplot(., aes(y = n, x = src, fill=tagged)) +
  geom_bar(stat = "identity", color="#595959") +
  ylim(0, 1500000) +
  ggtitle("Number of original Eng tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; tagged portions are green; N = ",
                           format(nrow(tweets),big.mark = ","))) +
  scale_fill_manual(values = c("FALSE"="#595959", "TRUE"="#77E197")) +
  theme(text = element_text(size = 16), legend.position = "none") +
  geom_label(aes(src, total, fill=NULL, label=format(round(total/1000, 1), big.mark = ",")), data=tots, vjust=-0.1, size=5)

print(plot_tweets)
  

tweets %>%
  group_by(src) %>%
  summarise(n_users = n_distinct(user_screen_name)) %>%
  ggplot(., aes(y = n_users, x = src)) +
  geom_bar(stat = "identity") +
  ylim(0, 850000) +
  geom_label(aes(label=format(n_users, big.mark = ",")), size = 4, vjust =-0.1) +
  ggtitle("Number of users of original Eng tweets",
          subtitle = paste("Re-tweets were excluded; N = ",
                           format(n_distinct(tweets$user_screen_name),
                                  big.mark = ","))) +
  theme(text = element_text(size = 14))
```


```{r generating users statistics}
if (file.exists(file.path(".", "usn_statistics.csv")))
{
  # Load the users statistics to save time
  usn = read_csv(file.path(".", "usn_statistics.csv"),
                 col_types = cols(src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
  
} else {
  # Make the user statistics and save it to disk
  usn = tweets %>%
    group_by(src, user_screen_name) %>%
    count(user_screen_name) %>%
    arrange(desc(n)) %>%
    ungroup(user_screen_name) %>%
    slice_max(n, n = 25) %>%
    ungroup
  
  write.csv(usn, file = file.path(".", "usn_statistics.csv"),
            row.names = FALSE)
}
# 
# # Visualise
# # TODO: combine these plots into smaller grid ones
# for (val in unique(usn$src))
# {
#   data = usn %>% filter(src == val)
#   
#   data %>%
#     select(user_screen_name, n)
#   plt =
#     ggplot(data, aes(x = reorder(user_screen_name, -n), y=n)) +
#     geom_bar(stat = "identity") +
#     ggtitle(paste("Top 25 tweeting frequencies by users in", val)) +
#     xlab("users") +
#     ylim(0, 7500) +
#     easy_remove_x_axis(what = "text")
#     
# 
#   print(plt)
#   rm(plt)
# }

```

```{r functions definition}
# Unify 5g synonyms
five5_synonyms = c("5g,","5g.","5g?")
unify_5g = function(x)
{
  stringi::stri_replace_all_fixed(str = x,
                                    five5_synonyms,
                                    "5g",
                                    vectorise_all = F)
}

unify_5g2 = function(x) gsub("\\b5g\\b", " 5g ", x)


# Unify coronavirus synonyms
# Pay attention that the order is critical
coronavirus_synonyms = c("coronavirusoutbreak",
                        "coronavirus19",
                        "corona virus",
                        "covid 19",
                        "covid19",
                        "corona ",
                        "covid")

unify_coronavirus = function(x)
{
  stringi::stri_replace_all_fixed(str = x,
                                  coronavirus_synonyms,
                                  "coronavirus",
                                  vectorise_all = F)
}

replacePunctuation = content_transformer(function(x) {
  return (gsub("[[:punct:]]"," ", x))
  })

set.seed(13712)
  
# Run the analysis for the combination of datasets
preprocess_texts = function(texts, marker)
{
  # Communicate what basic preprocessing did:
  print(paste(
    "URLs, smilies, emojies and mentions had been removed from dataset:",
    marker))
  
  # Some cleaning:
  
  # Lower-casing
  texts  = tm_map(texts, content_transformer(tolower))
  print(paste("Lowercasing done for dataset:", marker))
  
  # Remove hash-tags
  texts = tm_map(texts, content_transformer(replace_hash))
  print(paste("Hashtags removed from dataset:", marker))
  
  # Replace punctuation with spaces
  texts = tm_map(texts, replacePunctuation)
  print(paste("Punctuation replaced with spaces in dataset:", marker))
  
  # Unify 5G variants
  # texts = tm_map(texts, content_transformer(unify_5g))
  texts = tm_map(texts, content_transformer(unify_5g2))
  print(paste("5g variants (5g! and the likes) were standardised in", marker))
  
  # Remove stand-alone numbers to get rid of 19
  # # function source: https://stackoverflow.com/a/23866586/3429115
  remove_alone_nbr = function (x)
    gsub('\\s*(?<!\\B|-)\\d+(?!\\B|-)\\s*', " ", x, perl=TRUE)
  texts = tm_map(texts, content_transformer(remove_alone_nbr))
  print(paste("Standalone numbers removed from dataset:", marker))
  
  # Unify Coronavirus synonyms
  texts = tm_map(texts, content_transformer(unify_coronavirus))
  print(paste("Coronavirus synonyms were unified in",
              marker))
  print(coronavirus_synonyms)
  
  # Removing English stop-words
  texts = tm_map(texts, removeWords, stopwords("english"))
  print(paste("English stopwords removed from dataset:", marker))
  
  # Removing excessive white spaces
  texts = tm_map(texts, stripWhitespace)
  print(paste("White spaces normalised for dataset:", marker))
  
  # Stemming the terms
  texts = tm_map(texts, stemDocument, language = "english")
  print(paste("Snowball Stemming done for dataset:", marker))
  
  return(texts)
}

compute_salience = function(tdm.all, tdm.5g, verbose = F)
{
  if(verbose)
  {
    print(tdm.all)
    print(tdm.5g)
  }
  
  t.and.fiveG.DocCounts = (rowSums(as.matrix(tdm.5g) > 0))
  
  t.counts = (rowSums(as.matrix(
    tdm.all[tdm.all$dimnames$Terms %in% names(t.and.fiveG.DocCounts),]) > 0)
  )
    
  # Supply deleted term counts due to sparsity processing with NA
  t.counts = t.counts[names(t.and.fiveG.DocCounts)]
  
  fiveG.counts = as.double(length(tdm.5g$dimnames$Docs))
  tweets.counts = as.double(length(tdm.all$dimnames$Docs))
  
  # Calculate the conditional probabilities
  Pr.t.given.5g = t.and.fiveG.DocCounts / fiveG.counts
  Pr.t = t.counts / tweets.counts
  Pr.boost = Pr.t.given.5g - Pr.t
  
  return(cbind(Pr.t, Pr.t.given.5g, Pr.boost))
}
```

# 5G analysis

```{r}
smry = summary(tweets$five_g)
```

There are `r round(100 * as.numeric(smry["TRUE"]) / nrow(tweets), 2)`% 5G tweets in the hydrated datasets.

```{r, fig.height=12}
plt1 =
  tweets %>%
  filter(!five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/1000, 1),
                                            big.mark = ",")),
             size = 4, vjust = -0.1) +
  ggtitle("Number of non-5G tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; N =",
                           format(nrow(subset(tweets, !five_g)), big.mark=","))) +
  ylim(0, 2100000) +
  easy_remove_y_axis() +
  theme(text = element_text(size = 16)) +
  easy_text_size(which = "plot.title", size = 13) +
  easy_text_size(which = "plot.subtitle", size = 12)

plt2 = 
  tweets %>%
  filter(five_g) %>%
  group_by(src) %>%
  ggplot(., aes(x = src)) +
  geom_bar() +
  geom_label(stat='count', aes(label=format(round(after_stat(count)/1000, 1),
                                            big.mark = ",")),
             size = 4, vjust = -0.1) +
  ggtitle("Number of 5G tweets hydrated (in Thousands)",
          subtitle = paste("Re-tweets were excluded; N =",
                           format(nrow(subset(tweets, five_g)),big.mark=","))) +
  ylim(0, 2100000) +
  easy_remove_y_axis()  +
  theme(text = element_text(size = 16)) +
  easy_text_size(which = "plot.title", size = 13) +
  easy_text_size(which = "plot.subtitle", size = 12)

mddl_row = plot_grid(plt1, plt2)

plt3 =
  tweets %>%
  group_by(src) %>%
  summarise(fraction = sum(five_g)/n()) %>%
  ggplot(., aes(x = src, y = fraction)) +
  geom_bar(stat = "identity") +
  # geom_text(stat="identity", aes(label=paste0(round(100*fraction, 2), "%")),
  #           size = 5, hjust = 0.5, vjust = 1,
  #           position = position_dodge2(width = 0), colour = "white") +
  ylim(0, 0.002) +
  geom_label(stat='identity', aes(label=paste0(round(100*fraction, 2), "%")),
             size = 5, vjust = -0.1) +
  ggtitle("Percentage of 5G tweets hydrated",
          subtitle = paste0("Re-tweets were excluded; Overall percentage = ",
                           round(100 * as.numeric(smry["TRUE"]) / nrow(tweets),2),
                           "%"))+
  theme(text = element_text(size = 16))+
  easy_remove_y_axis() +
  easy_text_size(which = "plot.title", size = 13) +
  easy_text_size(which = "plot.subtitle", size = 12)

plot_grid(plot_tweets + easy_remove_y_axis(),
          mddl_row,
          plt3,
          ncol = 1)

rm(plot_tweets, plt1, plt2, plt3, mddl_row)
# knitr::knit_exit()
```

Concentrating on 5g Tweets after preprocessing:

```{r combined dataset analysis}
five_g_tweets = tweets %>%
  filter(five_g)

#rm(tweets)

set.seed(13712)


val="all_combined"
if(RESAVE_TDMs)
{
  texts = VCorpus(VectorSource(five_g_tweets$text))
  texts = preprocess_texts(texts, val)
  # Building the term-doc matrix
  dtm_tweets = TermDocumentMatrix(texts,
                                  control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_tweets, file = file.path(DIR_TDMs_PATH,
                                                paste0(val,"_pure_5G",
                                                       MAX_SPARSITY,".rds")))
  
  # MEMORY INSUFFICIENT!
  all_texts = VCorpus(VectorSource(tweets$text))
  all_texts = preprocess_texts(all_texts, val)
  # Building the term-doc matrix
  dtm_all_tweets = TermDocumentMatrix(all_texts,
                                      control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_all_tweets, file = file.path(DIR_TDMs_PATH,
                                                paste0(val, "_pure_",MAX_SPARSITY,".rds")))
  
} else {
  
  # Load the TDM
  dtm_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                                paste0(val,"_pure_5G",
                                                       MAX_SPARSITY,".rds")))
  dtm_all_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                                paste0(val, "_pure_",MAX_SPARSITY,".rds")))
}

m <- as.matrix(dtm_tweets)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=unname(v))

vDoc = sort(rowSums(m > 0),decreasing=TRUE)
dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))

p = ggplot(d[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("occurrences") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in", val, "(Excluding Hashtags)")) +
coord_flip()
print(p)

p = ggplot(dDoc[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("tweets") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in", val, "(Excluding Hashtags)")) +
coord_flip()
print(p)
rm(p)

print("Pearson Correlation among 5g and other topics (threshold = 0.1):")
print(findAssocs(dtm_tweets, terms = c("5g"), corlimit = .1))

set.seed(13712)
# 
# layout(matrix(c(1, 2), nrow=2), heights=c(10, 90))
# par(mar=rep(0, 4))
# plot.new()
# text(x=0.5, y=0.5, val, font = 2, cex = 1)
# Skip 5G and Coronavirus terms
dDoc = dDoc[!dDoc$word %in% c("coronavirus", "5g"), ]
print("5G and Coronavirus terms are removed")
wordcloud(words = dDoc$word, freq = dDoc$freq, min.freq = 3, random.order=FALSE,
          max.words=200, rot.per=0.35, colors=brewer.pal(8, "Dark2"))

#rm(dtm_tweets, dtm_all_tweets)
```

```{r salience computation, fig.width=8, fig.height=5}
print("Salience and conditional probabilities after sufficing with 99% sparsity:")
dtm_all_tweets = removeSparseTerms(dtm_all_tweets, sparse = MAX_SPARSITY)
salience = compute_salience(tdm.all = dtm_all_tweets, tdm.5g = dtm_tweets)
# Exclude "5g" if there
if("5g" %in% names(salience))
  salience["5g",] = NA
# ggplot(data = data.frame(salience),
#        aes(x = "Pr.boost",y = rownames(salience))) +
#   geom_bar(stat = "identity")
  
barplot(sort(salience[, "Pr.boost"], decreasing = T)[1:30],
        las = 2, cex.names = 0.8
        #, cex.main = 3,
        # main ="Top 30 topics whose conditional prob. to occur\nin a tweet given '5g' exceed their marginal prob."
        )
```

# Non-5G

```{r Non-5G analyses}
non_5g_tweets = tweets %>%
  filter(!five_g)

# Treat n5G individually because it is idiosyncratic to this case only
n5g_fp = file.path(DIR_TDMs_PATH, paste0(val,"_pure_n5G", MAX_SPARSITY, ".rds"))

if(!file.exists(n5g_fp))
{
  n5g_texts = VCorpus(VectorSource(non_5g_tweets$text))
  n5g_texts = preprocess_texts(n5g_texts, val)
  # Building the term-doc matrix
  dtm_n5g_tweets = TermDocumentMatrix(n5g_texts,
                                      control = list(wordLengths=c(2, Inf)))
  saveRDS(object = dtm_n5g_tweets, file = n5g_fp)
  print("TDM for n5G created.")
} else {
  dtm_n5g_tweets = readRDS(file = n5g_fp)
  print("TDM for n5G loaded.")
}
```

```{r unpacking combined data -- n5G}
val = "non_5G"
dtm_n5g_tweets = removeSparseTerms(x = dtm_n5g_tweets, sparse = MAX_SPARSITY)

# Make the general wordcloud for non 5g terms
m <- as.matrix(dtm_n5g_tweets)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=unname(v))

vDoc = sort(rowSums(m > 0),decreasing=TRUE)
dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))

p = ggplot(d[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("occurrences") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in Non-5G Tweets (Excluding Hashtags)")) +
coord_flip()
print(p)

ggsave(filename = paste0("wc_analysis_CorpusOcurrences",val,".pdf"),
         path = "./figures/")

p = ggplot(dDoc[1:50, ],
     aes(x = reorder(word, freq), y = freq)) +
geom_bar(stat = "identity") +
xlab("frequent words") +
ylab("tweets") +
#easy_rotate_x_labels(angle = 90) +
ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in Non-5G Tweets (Excluding Hashtags)")) +
coord_flip()

ggsave(filename = paste0("wc_analysis_TweetOcurrences",val,".pdf"),
         path = "./figures/")

print(p)
rm(p)

print("removing coronavirus term: done")
dDoc = dDoc[!dDoc$word == "coronavirus", ]
wordcloud(words = dDoc$word, freq = dDoc$freq,
          min.freq = 3, random.order=FALSE,
          max.words=200, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
rm(m, v, d, dDoc, vDoc)
gc()
```

---

```{r iterating over datasets}
# Iterate to produce the word-clouds all of the datasets one by one
# SAve the 50 most frequent terms in each dataset
freq.terms = data.frame()
freq.terms.all = data.frame()

for (val in unique(tweets$src))
{
  if(RESAVE_INDIVIDUAL_TDMs)
  {
    texts = VCorpus(VectorSource(subset(five_g_tweets, src == val)$text))
    texts = preprocess_texts(texts, val)
    # Building the term-doc matrix
    dtm_tweets = TermDocumentMatrix(texts,
                                    control = list(wordLengths=c(2, Inf)))
    saveRDS(object = dtm_tweets, file = file.path(DIR_TDMs_PATH,
                                                  paste0(val,"_pure_5G", MAX_SPARSITY,".rds")))

    # All tweets now
    texts = VCorpus(VectorSource(subset(tweets, src == val)$text))
    texts = preprocess_texts(texts, val)
    # Building the term-doc matrix
    dtm_all_tweets = TermDocumentMatrix(texts,
                                        control = list(wordLengths=c(2, Inf)))
    saveRDS(object = dtm_all_tweets, file = file.path(DIR_TDMs_PATH,
                                                      paste0("_pure_", val, MAX_SPARSITY,".rds")))
  } else {
    # Loading the term-doc matrices
    dtm_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                                  paste0(val,"_pure_5G", MAX_SPARSITY,".rds")))
    dtm_all_tweets = readRDS(file = file.path(DIR_TDMs_PATH,
                                                      paste0("_pure_", val, MAX_SPARSITY,".rds")))
  }
  
  # Run all tweets analysis
  print(paste0("Sustaining less than ", 100*MAX_SPARSITY, "% sparsity"))
  dtm_all_tweets = removeSparseTerms(dtm_all_tweets, sparse = MAX_SPARSITY)
  m = as.matrix(dtm_all_tweets)
  # Corpus counts
  v = sort(rowSums(m),decreasing=TRUE)
  d = data.frame(word = names(v),freq=unname(v))
  
  # Tweets counts
  vDoc = sort(rowSums(m > 0),decreasing=TRUE)
  dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))
  dDoc[, "src"] = val
  dDoc[, "size"] = nrow(subset(tweets, src == val))
  # Append the frequent terms to the repository
  if(nrow(freq.terms.all) == 0)
    freq.terms.all = dDoc
  else
    freq.terms.all = rbind(freq.terms.all, dDoc)
  
  # Run 5G analysis
  m = as.matrix(dtm_tweets)
  # Corpus counts
  v = sort(rowSums(m),decreasing=TRUE)
  d = data.frame(word = names(v),freq=unname(v))
  
  # Tweets counts
  vDoc = sort(rowSums(m > 0),decreasing=TRUE)
  dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))
  dDoc[, "src"] = val
  dDoc[, "size"] = nrow(subset(five_g_tweets, src == val))
  # Append the frequent terms to the repository
  if(nrow(freq.terms) == 0)
    freq.terms = dDoc
  else
    freq.terms = rbind(freq.terms, dDoc)

  p = ggplot(d[1:50, ],
       aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity") +
  xlab("frequent words") +
  ylab("occurrences") +
  #easy_rotate_x_labels(angle = 90) +
  ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in", val, "(Excluding Hashtags)")) +
  coord_flip()
  print(p)

  p = ggplot(dDoc[1:50, ],
       aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity") +
  xlab("frequent words") +
  ylab("tweets") +
  #easy_rotate_x_labels(angle = 90) +
  ggtitle(paste("Top 50 Frequent Words Tweet Occurrences in ", val, "(Excluding Hashtags)")) +
  coord_flip()
  print(p)
  rm(p)

  
  print("Pearson Correlation scores among 5g term and other topics, with a strength of 0.2 and above:")
  print(findAssocs(dtm_tweets, terms = c("5g"), corlimit = .2))
  
  print("Salience and conditional probabilities after sufficing with 99% sparsity:")
  dtm_all_tweets = removeSparseTerms(dtm_all_tweets, sparse = MAX_SPARSITY)
  salience = compute_salience(tdm.all = dtm_all_tweets, tdm.5g = dtm_tweets)
  # Exclude "5g" if there
  if("5g" %in% rownames(salience))
    salience["5g",] = NA
  # ggplot(data = data.frame(salience),
  #        aes(x = "Pr.boost",y = rownames(salience))) +
  #   geom_bar(stat = "identity")
  
  barplot(sort(salience[, "Pr.boost"], decreasing = T)[1:30],
          las = 2, cex.names = 1.0
          # , cex.main = 3,
          # main ="Top 30 topics whose conditional prob. to occur\nin a tweet given '5g' exceed their marginal prob."
          )
  # title(main=ttl, cex.main=0.6)
  
  # layout(matrix(c(1, 2), nrow=2), heights=c(10, 90))
  # par(mar=rep(0, 4))
  # plot.new()
  # text(x=0.5, y=0.5, val, font = 2, cex = 1)
  d = dDoc[!dDoc$word %in% c("coronavirus", "5g"), ]
  wordcloud(words = d$word, freq = d$freq, min.freq = 3,
            max.words=200, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  
  gc()
}

# Save the frequencies to disk
DIR_FREQ_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "freqterms")
saveRDS(object = freq.terms, file = file.path(DIR_FREQ_PATH, "_pure_freq_terms.rds"))
saveRDS(object = freq.terms.all, file = file.path(DIR_FREQ_PATH, "_pure_freq_terms_all.rds"))

```

We have calculate the probabilities of a term \(w\) given the term \(5g\):

\[P(w | 5g) = \frac{P(w \cap 5g)}{P(5g)} = \frac{C(tweets_{w\wedge5g})}{C(tweets_{5g})}\]

Where:

\[P(w \cap 5g) = \frac{C(tweets_{w\wedge5g})}{C(tweets_{all})}\]
\[P(5g) = \frac{C(tweets_{5g})}{C(tweets_{all})}\]

**This is the salience**.

---

# Evolution Analysis

```{r visulaising evolution function}
visualise_evolution = function(terms_frequencies, label, freq_threshold=15)
{
  # terms_frequencies should be a dataframe of word and frequencies
  # with factorial source and size of source     
  if(!is.factor(terms_frequencies$src))
  {
    terms_frequencies$src = factor(x = terms_frequencies$src,
                                   ordered = TRUE,
                                   levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                              "Apr01", "Apr15","May01"))
  }
  
  
  # Now we will choose the top n words, excluding words that appear 1 time only:
  topN = terms_frequencies %>%
    group_by(src) %>%
    top_n(n = freq_threshold, wt = freq) %>%
    ungroup
  
  # Remove one-time words
  topN = topN[topN$freq > 1, ]
  
  # Get all terms data from the dataset, to revive those which weren't frequent in some of them, so that we study them nonetheless
  topN = terms_frequencies[terms_frequencies$word %in% topN$word,]
  
  # Append the local percentages
  topN[, "percentage"] = 100*topN$freq/topN$size
  
  # Append the global total percentages in all datasets
  total = topN %>%
    select(src, size) %>%
    unique %>%
    summarise(sum(size)) %>%
    pull
  htgsStats = topN %>%
    group_by(word) %>%
    summarise(totfreq = sum(freq)) %>%
    ungroup %>%
    mutate(tot_percentage = 100*totfreq / total)
  
  # Merge the two datasets so that we get all the data in one place
  topN = merge(x = topN, y=htgsStats)
  
  # Re-use the code we have developed before for mere terms on the hashtags:
  evol_plot = topN %>%
    mutate(source = factor(topN$src,
                           levels = rev(levels(topN$src)))) %>%
    ggplot(data=.,
           aes(y = reorder(
             paste0(word, " (",round(tot_percentage,2),"%)")
             , tot_percentage),
               x=percentage, fill=source)) +
    geom_bar(stat="identity", position = "fill", colour = "black") +
    xlab("occurring percentage of terms in releavant tweet datasets") +
    ylab("hashtag") +
    # ggtitle(paste("Percentage compositions of the top",
    #               freq_threshold,
    #               "hashtags in",label,"tweets across datasets and how they evolve"),
    #         subtitle = "Numbers in parantheses are total percentage of tweets containing the tag in all datasets combined") +
    guides(fill = guide_legend(reverse = T, override.aes=list(shape=21))) +
    scale_fill_manual(values = c("#381A15","#5E3341","#6C5979","#5489A5",
                                 "#36B8B0","#77E197","#E4FE75")) +
    theme(text = element_text(size = 16))
  
  # ggsave(filename = paste0("Term_Evolution_", label, ".pdf"),
  #        path = "./figures/", device = "pdf")
  print(evol_plot)
  
  gc()
  return(topN)
}
```

```{r visualising evolution, fig.width=10, fig.height=15}
retAll = visualise_evolution(terms_frequencies = freq.terms.all, label = "All",
                          freq_threshold = 10)
ret5G = visualise_evolution(terms_frequencies = freq.terms, label = "5G",
                          freq_threshold = 10)
```


---
