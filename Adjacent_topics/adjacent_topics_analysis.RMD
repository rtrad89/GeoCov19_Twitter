---
title: "Adjacent Topics to 5G COVID-19 Tweets"
author: "RTRAD"
date: "`r Sys.Date()`"
output: 
  html_document: 
    fig_width: 10
    fig_height: 10
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      dev = "svg")

# Libraries
library(readr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggeasy)
library(tm)
library(wordcloud)
library(textclean)
library(cowplot)

options(error = function() {beepr::beep(9)})

theme_set(theme_linedraw())
theme_update(plot.title = element_text(hjust = 0.5),
             plot.subtitle = element_text(hjust = 0.5))
          
# Control variable
RECOMBINE = FALSE
DIR_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "annotated")
FNAME = "annotated_tweets.csv"
FPATH = file.path(DIR_PATH, FNAME)


RESAVE_TDMs = FALSE # TRUE takes a LOT of time!
DIR_TDMs_PATH = file.path("..", "..", "..",
                     "Datasets","twitter-sars-cov-2",
                     "preprocessed_tdms")

MAX_SPARSITY = 0.999
```

Loading the data:

```{r, make and load the data after skipping NA lables}
if (RECOMBINE)
{
  FNAME.Feb01 = "annotated_original_200201.csv"
  FPATH.Feb01 = file.path(DIR_PATH, FNAME.Feb01)
  
  FNAME.Feb15 = "annotated_original_200215.csv"
  FPATH.Feb15 = file.path(DIR_PATH, FNAME.Feb15)
  
  FNAME.Mar01 = "annotated_original_200301.csv"
  FPATH.Mar01 = file.path(DIR_PATH, FNAME.Mar01)
  
  FNAME.Apr01 = "annotated_original_200401.csv"
  FPATH.Apr01 = file.path(DIR_PATH, FNAME.Apr01)
  
  FNAME.May01 = "annotated_original_200501.csv"
  FPATH.May01 = file.path(DIR_PATH, FNAME.May01)

  FNAME.Mar15 = "annotated_original_200315.csv"
  FPATH.Mar15 = file.path(DIR_PATH, FNAME.Mar15)
  
  FNAME.Apr15 = "annotated_original_200415.csv"
  FPATH.Apr15 = file.path(DIR_PATH, FNAME.Apr15)
  
  fpaths = c(FPATH.Feb01, FPATH.Feb15, FPATH.Mar01, FPATH.Mar15,
             FPATH.Apr01, FPATH.Apr15, FPATH.May01)
  
  tweets = tibble::tibble()
  for (fp in fpaths)
  {
    temp = read_csv(fp,
                    col_types = cols(id = col_character()))
    temp$src = paste0(
      month.abb[as.numeric(
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 3, 4))
      ],
      substr(stringr::str_extract(basename(fp), pattern = "\\d+"), 5, 6)
    )
  
  if(nrow(tweets) == 0)
    tweets = temp
  else
    tweets = rbind(tweets, temp)
  
  rm(temp)
  }
  
  # Drop tweets where label is NA
  tweets = tweets[!is.na(tweets$five_g),]
  

  tweets$src = factor(x = tweets$src,
                      ordered = TRUE,
                      levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01"))
  
  # Save tweets to disk
  write_csv(tweets, file = file.path(DIR_PATH, FNAME))
} else {
  tweets = read_csv(FPATH, col_types = cols(
    id = col_character(),
    src = col_factor(ordered = TRUE,
                     levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                 "Apr01", "Apr15","May01")))
    )
}
```

Some users annotate their tweets with labels which represent the ulterior themes/topics. That valuable annotation is represented as what's called hashtags, and it would be very interesting to analyse and explore what topics are being discussed in COVID tweets.


In our combined `r round(nrow(tweets)/1000000, 1)`m tweets, there are `r round(nrow(subset(tweets, !is.na(hashtags)))/1000000, 1)`m annotated tweets whose hashtags we shall examine in a holistic fashion and then in individual data sets.

```{r holistic hashtags analysis}
print("All tweets:")
print(table(tweets$src))
tweets = tweets %>%
  select(hashtags, five_g, src) %>%
  filter(!is.na(hashtags))
print("Tagged tweets:")
print(table(tweets$src))
```

```{r functions for preprocessing}
# Unify coronavirus synonyms
# Pay attention that the order is critical
coronavirus_synonyms = c("coronavirus19",
                        "corona virus",
                        "covid 19",
                        "sarscov2",
                        "ncov2019",
                        "2019ncov",
                        "covid19",
                        "corona ",
                        "corona",
                        "covid",
                        "ncov")

unify_coronavirus = function(x)
{
  x2 = stringi::stri_replace_all_fixed(str = x,
                                  coronavirus_synonyms,
                                  "coronavirus",
                                  vectorise_all = F)
  
  x2 = stringi::stri_replace_all_fixed(str = x2,
                                  "virusvirus",
                                  "virus",
                                  vectorise_all = F)
  
  stringi::stri_replace_all_regex(str = x2,
                                  pattern = "\\bcoronavirus[\\w]+\\b",
                                  replacement = "coronavirus",
                                  vectorize_all = F)
}



unify_5g2 = function(x) gsub("\\b5g\\b", " 5g ", x)

replacePunctuationAggressively = content_transformer(function(x) {
  return (gsub("[^[:alnum:]]"," ", x))
  })

removeRedundantHashtags = function(x)
{
  # This has to be run while respecting tweets' document boundaries
  # IT CANNOT BE USED WHEN INTEGRATING ALL THE TEXTS IN ONE DOCUMENT
  words = unlist(strsplit(x, split = " "))
  return(paste(unique(words), collapse = " "))
  
}


preprocess_hashtags = function(texts, marker)
{
  # Lower-casing
  texts  = tm_map(texts, content_transformer(tolower))
  print(paste("Lowercasing done for dataset:", marker))
  
  # Replace punctuation with spaces
  texts = tm_map(texts, replacePunctuationAggressively)
  print(paste("Punctuation replaced with spaces in dataset:", marker))
  
  # Unify 5G variants
  # texts = tm_map(texts, content_transformer(unify_5g))
  texts = tm_map(texts, content_transformer(unify_5g2))
  print(paste("5g variants (5g! and the likes) were standardised in", marker))
  
  # Unify Coronavirus synonyms
  texts = tm_map(texts, content_transformer(unify_coronavirus))
  print(paste("Coronavirus synonyms, redundant 'virus' and suffixes were unified in",
              marker))
  print(coronavirus_synonyms)
  
  # Removing excessive white spaces
  texts = tm_map(texts, stripWhitespace)
  print(paste("White spaces normalised for dataset:", marker))

  
  # Stemming the terms
  # texts = tm_map(texts, stemDocument, language = "english")
  # print(paste("Snowball Stemming done for dataset:", marker))
  
  # #Remove duplicate hashtags in a tweet
  # texts = tm_map(texts, content_transformer(removeRedundantHashtags))
  # print(paste("Redundant hashtags per tweet removed in dataset:", marker))
  
  return(texts)
}

vsm_modeller = function(data, RESAVE_TDMs, val, is5G,
                        rds_prefname = "Adjacent_Topics",
                        sparsity_threshold = NA,
                        corrterm = "coronavirus",
                        remove_corona_5g_tokens = TRUE)
{
  # 5G Suffix
  if(is.na(is5G))
  {
    five_G_suffix = "_ALL_"
  } else {
    five_G_suffix = paste0("_5G_", is5G)
  }
  
  if(RESAVE_TDMs)
  {
    # Build a TDM using the hashtags
    htgs = VCorpus(VectorSource(data$hashtags)
                   )
    htgs = preprocess_hashtags(htgs, val)
    # Building the term-doc matrix
    dtm_htgs = TermDocumentMatrix(htgs,
                                  control = list(wordLengths=c(2, Inf)))
    saveRDS(object = dtm_htgs, file = file.path(DIR_TDMs_PATH,
                                                paste0(rds_prefname, "_", val,
                                                       five_G_suffix,
                                                       MAX_SPARSITY,".rds")))
    } else {
    dtm_htgs = readRDS(file = file.path(DIR_TDMs_PATH,
                                        paste0(rds_prefname, "_", val,
                                               five_G_suffix,
                                               MAX_SPARSITY,".rds")))
  }
  # Save the sparse dtm matrix for statistical purposes
  ret = dtm_htgs
  
  # Remove sparsity if desired
  if(!is.na(sparsity_threshold))
  {
    # Due to high memory requirements, we have to remove some sparse terms
    # Normally this does not remove significant relations we hope to find
    dtm_htgs = removeSparseTerms(dtm_htgs, sparsity_threshold)
    print(paste0("NOTE: sparsity pruned, less than ",
                100*sparsity_threshold, "% is kept"))
  }
  
  if(RESAVE_TDMs)
  {
    m = as.matrix(dtm_htgs)
    
    # # Corpus counts
    # v = sort(rowSums(m),decreasing=TRUE)
    # d = data.frame(word = names(v),freq=unname(v))
    
    # Tweets counts
    vDoc = sort(rowSums(m > 0),decreasing=TRUE)
    dDoc = data.frame(word = names(vDoc),freq=unname(vDoc))
    
    rm(m, vDoc)
    saveRDS(object = dDoc, file = file.path(DIR_TDMs_PATH,
                                            paste0(rds_prefname, "_dDoc_",
                                                   val,
                                                    five_G_suffix,
                                                   MAX_SPARSITY,".rds")))
  } else {
    dDoc = readRDS(file = file.path(DIR_TDMs_PATH,
                                    paste0(rds_prefname, "_dDoc_",val,
                                            five_G_suffix,
                                           MAX_SPARSITY,".rds")))
  }

  # p = ggplot(d[2:51, ],
  #      aes(x = reorder(word, freq), y = freq)) +
  # geom_bar(stat = "identity") +
  # xlab("frequent words") +
  # ylab("occurrences") +
  # #easy_rotate_x_labels(angle = 90) +
  # ggtitle(paste("Top 50 Frequent Words Corpus Occurrences in", val, "Besides 'Coronavirus'")) +
  # coord_flip()
  # print(p)
  
  if(remove_corona_5g_tokens)
  {
    if(!is.na(is5G) & is5G)
    {
      dDoc = dDoc[!dDoc$word %in% c("coronavirus", "5g"), ] # Skip coronavirus and 5G terms
      mySubTitle = "'Coronavirus' and '5G' are skipped."
    } else {
      dDoc = dDoc[!dDoc$word == "coronavirus", ] # Skip coronavirus terms
      mySubTitle = "'Coronavirus' is skipped."
    }
  }
  else
    mySubTitle = "All tokens are included."
  
  p = ggplot(dDoc[1:25, ],
       aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity") +
  xlab("frequent words") +
  ylab("tweets") +
  #easy_rotate_x_labels(angle = 90) +
  ggtitle(paste("Top 25 Frequent Words Tweet Occurrences in", val),
          subtitle = mySubTitle) +
  coord_flip()
  print(p)
  #ggsave(filename = paste0(rds_prefname, val, ifelse(is5G, "_5G", "_n5G"), ".pdf"),
  #       path = "./figures/", device = "pdf")
  rm(p)
  
  print("Pearson Correlation among coronavirus and other terms (threshold = 0.1):")
  print(findAssocs(dtm_htgs, terms = c("coronavirus"), corlimit = .1))
  set.seed(13712)
  
  wordcloud(words = dDoc$word, freq = dDoc$freq, min.freq = 3,
            random.order=FALSE, max.words=100, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  
  # Return a vector of the DTMs, before and after sparsity removal
  gc()
  return(list(sparse=ret, pruned=dtm_htgs, termfreqs=dDoc))
}

visualise_evolution = function(terms_frequencies, label, freq_threshold=15)
{
  # terms_frequencies should be a dataframe of word and frequencies
  # with factorial source and size of source     
  if(!is.factor(terms_frequencies$src))
  {
    terms_frequencies$src = factor(x = terms_frequencies$src,
                                   ordered = TRUE,
                                   levels = c("Feb01", "Feb15", "Mar01", "Mar15",
                                              "Apr01", "Apr15","May01"))
  }
  
  
  # Now we will choose the top n words, excluding words that appear 1 time only:
  topN = terms_frequencies %>%
    group_by(src) %>%
    top_n(n = freq_threshold, wt = freq) %>%
    ungroup
  
  # Remove one-time words
  topN = topN[topN$freq > 1, ]
  
  # Get all terms data from the dataset, to revive those which weren't frequent in some of them, so that we study them nonetheless
  topN = terms_frequencies[terms_frequencies$word %in% topN$word,]
  
  # Append the local percentages
  topN[, "percentage"] = 100*topN$freq/topN$size
  
  # Append the global total percentages in all datasets
  total = topN %>%
    select(src, size) %>%
    unique %>%
    summarise(sum(size)) %>%
    pull
  htgsStats = topN %>%
    group_by(word) %>%
    summarise(totfreq = sum(freq)) %>%
    ungroup %>%
    mutate(tot_percentage = 100*totfreq / total)
  
  # Merge the two datasets so that we get all the data in one place
  topN = merge(x = topN, y=htgsStats)
  
  # Re-use the code we have developed before for mere terms on the hashtags:
  evol_plot = topN %>%
    mutate(source = factor(topN$src,
                           levels = rev(levels(topN$src)))) %>%
    ggplot(data=.,
           aes(y = reorder(
             paste0(word, " (",round(tot_percentage,2),"%)")
             , tot_percentage),
               x=percentage, fill=source)) +
    geom_bar(stat="identity", position = "fill", colour = "black") +
    xlab("occurring percentage of terms in releavant tweet datasets") +
    ylab("hashtag") +
    # ggtitle(paste("Percentage compositions of the top",
    #               freq_threshold,
    #               "hashtags in",label,"tweets across datasets and how they evolve"),
    #         subtitle = "Numbers in parantheses are total percentage of tweets containing the tag in all datasets combined") +
    guides(fill = guide_legend(reverse = T, override.aes=list(shape=21))) +
    scale_fill_manual(values = c("#381A15","#5E3341","#6C5979","#5489A5",
                                 "#36B8B0","#77E197","#E4FE75")) +
    theme(text = element_text(size=16))
  
  # ggsave(filename = paste0("Adjacent_Evolution_", label, ".pdf"),
  #        path = "./figures/", device = "pdf")
  print(evol_plot)
  
  gc()
  return(topN)
}


compute_salience = function(tdm.all, tdm.5g, verbose = F)
{
  if(verbose)
  {
    print(tdm.all)
    print(tdm.5g)
  }
  
  t.and.fiveG.DocCounts = (rowSums(as.matrix(tdm.5g) > 0))
  
  t.counts = (rowSums(as.matrix(
    tdm.all[tdm.all$dimnames$Terms %in% names(t.and.fiveG.DocCounts),]) > 0)
  )
    
  # Supply deleted term counts due to sparsity processing with NA
  t.counts = t.counts[names(t.and.fiveG.DocCounts)]
  
  fiveG.counts = as.double(length(tdm.5g$dimnames$Docs))
  tweets.counts = as.double(length(tdm.all$dimnames$Docs))
  
  # Calculate the conditional probabilities
  Pr.t.given.5g = t.and.fiveG.DocCounts / fiveG.counts
  Pr.t = t.counts / tweets.counts
  Pr.boost = Pr.t.given.5g - Pr.t
  
  return(cbind(Pr.t, Pr.t.given.5g, Pr.boost))
}

```

```{r}
# Split the tweets in 5g and non-5g
tweets.n5g = subset(x = tweets, subset = !five_g)
tweets.5g = subset(x = tweets, subset = five_g)
# If tweets are to be retained, e.g. for salience compuration, then comment the following
# rm(tweets)
```

# All Dataset Analysis
```{r make the full dataset as TDM}
# We will need to load the full data for the salience analysis
dtm_all = vsm_modeller(data = tweets, RESAVE_TDMs = RESAVE_TDMs,
                       val = "combined",
                       sparsity_threshold = MAX_SPARSITY, is5G= NA)
```

# Non-5G Analysis

```{r building the TDM for non-5G}
dtm_htgs = vsm_modeller(data = tweets.n5g, RESAVE_TDMs = RESAVE_TDMs,
                        val = "Non-5G",
                        sparsity_threshold = MAX_SPARSITY, is5G = FALSE)
```

As we see, the current term-document matrix holds `r dtm_htgs$sparse$nrow` terms in `r dtm_htgs$sparse$ncol` tweets.

`r ifelse(is.na(MAX_SPARSITY), print("") , print(paste0("Due to efficiency reason we have to suffice with the least sparse terms because we have a lot of sparsity in the TDM. That should not remove significant relations or findings we are after. Terms that are ", 100*MAX_SPARSITY ,"% sparse or more will be pruned away. Removing extremely sparse terms leaves us with ", dtm_htgs$pruned$nrow , " terms.")))`

In some tweets, a term is repeated more than once, the case we consider redundant. We suffice only with one occurrence, so eventually we are regarding the number of tweets in which the term appears, disregarding redundancies in a tweet. We explored the counts of these denser terms too above.

# 5G Tweets
The same thing is applied to 5G tweets, but without removing sparsity as there is no need to due to the smaller size of data.

```{r building the TDM for 5G}
dtm_htgs = vsm_modeller(data = tweets.5g, RESAVE_TDMs = RESAVE_TDMs,
                        val = "Non-5G",
                        sparsity_threshold = NA, is5G = TRUE)
```

The current term-document matrix holds `r dtm_htgs$sparse$nrow` terms in `r dtm_htgs$sparse$ncol` tweets.

## Salience of 5G hashtags

We will compute the salience of 5G tweets in our full dataset.

```{r compute the salince, fig.width=8, fig.height=5}
salience = compute_salience(tdm.all = dtm_all$pruned, tdm.5g = dtm_htgs$pruned)

# Exclude "5g" if there
if("5g" %in% rownames(salience))
  salience["5g",] = NA
  
barplot(sort(salience[, "Pr.boost"], decreasing = T)[1:30],
        las = 2, cex.names = 0.8)

```

# A Drill-Down on Each Dataset for Evolvement

We will run the same analysis on each dataset, studying non-5G tweets first then 5G.

```{r}
freq = data.frame()
freq.n5g = data.frame()
freq.5g = data.frame()

for(val in unique(tweets$src))
{
  print(paste("Studying", val, "dataset..."))
  print("All:")
  # Filter the dataset and load it into our modeller
  termfreq = vsm_modeller(data = subset(tweets, subset = src==val),
                              RESAVE_TDMs = RESAVE_TDMs, val = val, is5G = NA,
                              sparsity_threshold = MAX_SPARSITY,
                          remove_corona_5g_tokens = FALSE)[3]$termfreqs
  
  print("Non-5G:")
  # Filter the dataset and load it into our modeller
  termfreq.n5g = vsm_modeller(data = subset(tweets.n5g, subset = src==val),
                              RESAVE_TDMs = RESAVE_TDMs, val = val, is5G = FALSE,
                              sparsity_threshold = MAX_SPARSITY,
                              remove_corona_5g_tokens = FALSE)[3]$termfreqs
  print("5G:")
  termfreq.5g = vsm_modeller(data = subset(tweets.5g, subset = src==val),
                             RESAVE_TDMs = RESAVE_TDMs, val = val, is5G = TRUE,
                             sparsity_threshold = NA,
                             remove_corona_5g_tokens = FALSE)[3]$termfreqs
  
  # Append the freqs to the global evolution matrix
  termfreq[, "src"] = val
  termfreq[, "size"] = nrow(subset(tweets, subset = src==val))
  if(nrow(freq.n5g) == 0)
    freq = termfreq
  else
    freq = rbind(freq, termfreq)  
  
  termfreq.n5g[, "src"] = val
  termfreq.n5g[, "size"] = nrow(subset(tweets.n5g, subset = src==val))
  if(nrow(freq.n5g) == 0)
    freq.n5g = termfreq.n5g
  else
    freq.n5g = rbind(freq.n5g, termfreq.n5g)
  
  termfreq.5g[, "src"] = val
  termfreq.5g[, "size"] = nrow(subset(tweets.5g, subset = src==val))
  if(nrow(freq.5g) == 0)
    freq.5g = termfreq.5g
  else
    freq.5g = rbind(freq.5g, termfreq.5g)
}
```

```{r evolution visualisation}
print("Evolution of in all data:")
topN.all = visualise_evolution(terms_frequencies = freq, freq_threshold = 10,
                               label = "All")

print("Evolution of in 5G data:")
topN.5g = visualise_evolution(terms_frequencies = freq.5g, freq_threshold = 10,
                              label = "5G")

beepr::beep(3)
```


# Supplying the Missing Topics for Untagged Tweets?
We have 60% of tweets unlabelled. We shall use the state-of-the-art statistical modelling in order to infer the topical structures of these tweets via BTM.
